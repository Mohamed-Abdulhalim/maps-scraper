# Lead Finder â€“ Google Maps Scraper (Automated)

**Collect business leads from Google Maps.**  
Categories â†’ CSV â†’ Cleaned â†’ Upserted into Supabase â†’ Displayed in a web dashboard.

This repository contains an automated scraping pipeline that:

- Scrapes business listings from Google Maps using **Selenium + undetected_chromedriver**
- Cleans and normalizes data (phone formatting, address recovery, price extraction, duplicate photo detection)
- Pushes cleaned data to **Supabase** (upsert â€” no duplicates)
- Runs automatically on **GitHub Actions** (scheduled scraping)

> Built to solve a real problem: *finding clean contact data without manual search.*

---

## âœ¨ Features

| Feature | Details |
|--------|---------|
| âœ… Google Maps data extraction | Name, phone, website, address, rating, photos, categories |
| âœ… CSV cleaning pipeline | Fixes addresses, removes duplicates, normalizes phone numbers |
| âœ… Unique image extraction | Removes repeated Google Maps photo URLs across rows |
| âœ… Automated CI/CD pipeline | Scraper â†’ Cleaner â†’ Supabase push |
| âœ… Idempotent upsert | Avoids duplicated database entries |
| âœ… Secrets safe | No credentials committed into git |

---

## ğŸ› ï¸ Tech Stack

- **Python 3.11**
- `undetected-chromedriver`, `selenium`
- Supabase (PostgreSQL)
- GitHub Actions (CI/CD automation)

---

## ğŸ“‚ Repository Structure

```
maps-scraper/
â”‚
â”œâ”€â”€ maps.py             # Google Maps scraper â†’ results.csv
â”œâ”€â”€ csv_cleaner.py      # Cleans / normalizes scraped CSV
â”œâ”€â”€ supabase_push.py    # Upserts final CSV into Supabase
â”œâ”€â”€ categories.txt      # List of categories to scrape (one per line)
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ scrape.yml      # Scheduled GitHub Action (scrape + clean + push)
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env.example        # Environment variables (local only)
```

## âš™ï¸ Environment Variables

Create `.env`:

SUPABASE_URL=<your-project-url>
SUPABASE_SERVICE_ROLE=<service-role-key>
> Do NOT commit real keys â€” use **GitHub Secrets** for automation.

---

## ğŸš€ Run locally

```bash
py maps.py --categories-file categories.txt --location "Cairo, Egypt" --max-places 20 --output results.csv --headless

py csv_cleaner.py --in results.csv --out Cleaned.csv


py supabase_push.py Cleaned.csv
```
ğŸ•’ GitHub Actions (Auto-Scraping)

The repo includes a workflow that:

- Runs the scraper headlessly

- Cleans the results

- Pushes to Supabase

To trigger manually:
GitHub repo â†’ Actions â†’ Run workflow

ğŸ”¥ Roadmap

âœ… Dashboard UI (in progress)

â³ Export to Excel / Notion

â³ Paid version with filters & email enrichment

ğŸ“œ License

This project is licensed under MIT License.

ğŸ™‹â€â™‚ï¸ Author

Mohamed Abdulhalim

Data scraping, automation, Supabase & Python development.

LinkedIn: https://[www.linkedin.com/in/mohamed-abdulhalim](https://www.linkedin.com/in/halim99/)
