# Lead Finder â€“ Automated Google Maps Scraper & Lead Pipeline

A production-grade pipeline that turns Google Maps into clean, structured business leads.  
**Scrape â†’ Clean â†’ Enrich â†’ Upsert â†’ View in a live dashboard.**

Built for real-world use: scalable scraping, automated cleaning, deduplication, enrichment, and a live Supabase-powered UI deployed on Vercel.

---

## âœ¨ Highlights (Enterprise-Level Features)

### **Full Google Maps Extraction**
- Scrapes **names, phones, websites, addresses, ratings, reviews, photos, categories**, and detailed profile info.  
- Uses **Selenium + undetected-chromedriver**, rotating user-agents/languages, random delays, and periodic browser restarts to avoid bans.

### **Industrial-Grade Cleaning & Normalization**
- Standardizes **phone numbers**, **addresses**, **URLs**, and **social links**.  
- Extracts price data when available.  
- Removes duplicate businesses and duplicate photos.  
- Produces a clean, analysis-ready dataset.

### **Phone Enrichment Layer**
- Revisits profiles missing contact numbers and extracts phones from the business detail page.

### **Supabase Cloud Database (Upsert Logic)**
- Cleaned data is upserted into PostgreSQL (Supabase).  
- Unique key: **profile URL** â†’ no duplicates ever.  
- Batch inserts for performance.

### **CI/CD Automation**
- GitHub Actions runs the full pipeline on a schedule.  
- Automatically rotates cities each run.  
- Fully hands-off operation.

### **Live Dashboard UI**
- Flask backend + Vercel hosting.  
- Filter by category and location.  
- Fast pagination, mobile-friendly UI.

### **Security Built-In**
- No credentials in code.  
- `.env.example` included.  
- Production secrets stored in GitHub Secrets.

---

## ğŸ› ï¸ Tech Stack
- **Python 3.11**  
- Selenium, undetected-chromedriver  
- Supabase (PostgreSQL)  
- GitHub Actions  
- Flask (API + dashboard)  
- HTML / JS frontend (Vercel)

---

## ğŸ“‚ Project Structure

```
maps-scraper/
â”œâ”€â”€ maps.py                # Main scraper â†’ results.csv
â”œâ”€â”€ csv_cleaner.py         # Cleans & normalizes â†’ Cleaned.csv
â”œâ”€â”€ phone_enricher.py      # Fills missing phones â†’ Enriched.csv
â”œâ”€â”€ supabase_push.py       # Upserts into Supabase
â”œâ”€â”€ app.py                 # Flask API + dashboard
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html         # Dashboard UI
â”œâ”€â”€ categories.txt
â”œâ”€â”€ requirements.txt
â””â”€â”€ .github/workflows/
    â””â”€â”€ scrape.yml         # CI/CD pipeline
```

---

## âš™ï¸ Local Setup

### **1. Install dependencies**
```bash
pip install -r requirements.txt
```

### **2. Create `.env`**
```
SUPABASE_URL=...
SUPABASE_SERVICE_ROLE=...
```

### **3. Run the pipeline**
```bash
py maps.py --categories-file categories.txt --location "Cairo, Egypt" --max-places 20 --output results.csv --headless
py csv_cleaner.py --in results.csv --out Cleaned.csv
py phone_enricher.py --in Cleaned.csv --out Enriched.csv
py supabase_push.py Enriched.csv
```

### **4. Run the dashboard**
```bash
export SUPABASE_URL=...
export SUPABASE_ANON_KEY=...
py app.py
```

Visit: **http://localhost:5000**

---

## ğŸ•’ Automation (GitHub Actions)

The scheduled workflow handles:

- Headless scraping  
- Cleaning  
- Phone enrichment  
- Upsert to Supabase  
- Automatic city rotation  

Manual triggering:  
**GitHub â†’ Actions â†’ Run workflow**

---

## ğŸ’¡ Use Cases
- Sales lead generation  
- Local business intelligence  
- Market research  
- Directory apps & location-based platforms  
- Automated client data collection for agencies/consultants

---

## ğŸ”® Roadmap
- Excel / Notion export  
- Email enrichment  
- Rating / â€œopen nowâ€ filters  
- Multi-region scraping  
- Advanced analytics dashboard  

---

## ğŸ“œ License
MIT License.

---

## ğŸ™‹â€â™‚ï¸ Author
**Mohamed Abdulhalim**  
Data scraping, automation, Supabase & Python development.  
LinkedIn: https://www.linkedin.com/in/halim99/
